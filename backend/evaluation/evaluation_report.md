# Urdu Trigram Language Model — Hyperparameter Evaluation Report

## 1. Model Overview
This report documents the hyperparameter sweep performed on a **trigram language model** trained on an Urdu story corpus using **Maximum Likelihood Estimation (MLE)** with **linear interpolation**.

### Interpolation Formula
$$
P_{interp}(t_i | t_{i-2}, t_{i-1}) = \lambda_3 \cdot P(t_i | t_{i-2}, t_{i-1}) + \lambda_2 \cdot P(t_i | t_{i-1}) + \lambda_1 \cdot P(t_i)
$$
where $\lambda_1 + \lambda_2 + \lambda_3 = 1.0$
- $\lambda_1$ = unigram weight
- $\lambda_2$ = bigram weight
- $\lambda_3$ = trigram weight

### Special Tokens
| Token | Unicode  | Meaning           | Role in generation         |
|-------|----------|-------------------|---------------------------|
| EOS   | U+0003   | End of sentence   | Sentence boundary marker  |
| EOP   | U+0004   | End of paragraph  | Paragraph boundary marker |
| EOT   | U+0005   | End of story      | **Terminates generation** |

---

## 2. Corpus & Tokenizer Statistics
| Property                    | Value         |
|-----------------------------|---------------|
| BPE Vocabulary size         | 250           |
| Total lambda combinations   | 171           |
| Validation split            | Last 10%      |
| Sweep step size             | 0.05          |
| Worst perplexity seen       | 69.8426       |
| Best perplexity seen        | 20.0387       |

---

## 3. Optimal Hyperparameters
**λ1 (unigram):** 0.050  
**λ2 (bigram):** 0.250  
**λ3 (trigram):** 0.700  
**Validation Perplexity:** 20.0387

_These weights were selected by exhaustive grid search minimising held-out perplexity on the last 10% of the corpus._

---

## 4. Top-10 Lambda Combinations
| Rank | λ1    | λ2    | λ3    | Perplexity |
|------|-------|-------|-------|------------|
| 1    | 0.050 | 0.250 | 0.700 | 20.0387    |
| 2    | 0.050 | 0.300 | 0.650 | 20.0731    |
| 3    | 0.050 | 0.200 | 0.750 | 20.1327    |
| 4    | 0.050 | 0.350 | 0.600 | 20.2128    |
| 5    | 0.100 | 0.200 | 0.700 | 20.3803    |
| 6    | 0.100 | 0.250 | 0.650 | 20.3819    |
| 7    | 0.050 | 0.150 | 0.800 | 20.4022    |
| 8    | 0.050 | 0.400 | 0.550 | 20.4473    |
| 9    | 0.100 | 0.300 | 0.600 | 20.5027    |
| 10   | 0.100 | 0.150 | 0.750 | 20.5321    |

---

## 5. Evaluation Plots
- **Perplexity Heatmap (λ2 vs λ3, λ1 fixed at 0.05):** ![Heatmap](heatmap_l2_l3.png)
- **Perplexity vs Trigram Weight (λ3):** ![PP vs l3](perplexity_vs_l3.png)
- **Perplexity vs Bigram Weight (λ2):** ![PP vs l2](perplexity_vs_l2.png)
- **Top-20 Combinations:** ![Top 20](top20_combinations.png)
- **Lambda Space Scatter:** ![Scatter](lambda_scatter.png)

---

## 6. Sample Generation with Optimal Weights
Generated text using prompt `"ایک دفعہ"` with optimal lambda weights:

```
ایک دفعہد کی مد کو چھڑی سے پوچھا کہ تم اتنے سے پہلے کر آنے والے ان کا استعالیہ بیگم ہوتے ہی اس نے وہاں سے چلا گیا اور اچھا ٹھیک ہے۔  ￼  بات بتایا۔  ￼  ات کی قینچا۔  ￼  اگر ایسا بچہ تھا، جسمندہ رہی۔  ￼یہ دونوں بعد اسے احساس دلچسپی محبت سے 
```

---

## 7. Interpretation
**Why does λ2 (bigram) often dominate?**
- With a 250-token BPE vocabulary, the corpus produces ~144K unique trigrams out of a possible 250³ = 15.6M. Most trigram contexts are therefore sparse (seen once or never). The bigram distribution (~18K unique entries) provides a much denser, more reliable signal. The optimal λ weights reflect this: trigram precision is useful only when context has been seen before, and the interpolation formula handles the fallback gracefully.

**EOT-terminated generation**
- The model generates tokens until the EOT token (U+0005) is produced or the max-token cap is reached. EOS (U+0003) and EOP (U+0004) are treated as regular vocabulary items, so the model naturally learns sentence and paragraph boundary statistics from the corpus.

---

*Generated by evaluate.py — Urdu Story Generation Project*
